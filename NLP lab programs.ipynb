{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        " text = \"Hello! This is a simple example, and it works without libraries.\"\n",
        "stop_words = [\"is\", \"a\", \"and\", \"it\", \"the\", \"this\", \"to\", \"of\"]\n",
        "punctuation_marks = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "clean_text = \"\"\n",
        "for char in text:\n",
        "  if char not in punctuation_marks:\n",
        "    clean_text += char\n",
        "tokens = clean_text.split()\n",
        "filtered_tokens = []\n",
        "for word in tokens:\n",
        "  if word.lower() not in stop_words:\n",
        "    filtered_tokens.append(word)\n",
        "print(f\"Original Text: {text}\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"StopWords: {filtered_tokens}\")"
      ],
      "metadata": {
        "id": "F6UitSOX0M5q",
        "outputId": "85b827ab-5cb4-4260-ead5-f0769d826c72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Hello! This is a simple example, and it works without libraries.\n",
            "Tokens: ['Hello', 'This', 'is', 'a', 'simple', 'example', 'and', 'it', 'works', 'without', 'libraries']\n",
            "StopWords: ['Hello', 'simple', 'example', 'works', 'without', 'libraries']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:3: SyntaxWarning: invalid escape sequence '\\,'\n",
            "<>:3: SyntaxWarning: invalid escape sequence '\\,'\n",
            "/tmp/ipython-input-3575028423.py:3: SyntaxWarning: invalid escape sequence '\\,'\n",
            "  punctuation_marks = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "DoDz7PaG13Oi",
        "outputId": "56b64ee1-6d12-493b-c65c-6b2e0892967e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"balayya is not a name, it is an emotion. Jai balayya\"\n",
        "words=word_tokenize (sentence)\n",
        "word='emotion'\n",
        "sense=lesk (words, word)\n",
        "if sense:\n",
        "  print (f\"best sense for '(word)': {sense.name()}\")\n",
        "  print(f\"definition: {sense.definition()}\")\n",
        "else:\n",
        "  print(f\"no sense found for '{word}'.\")"
      ],
      "metadata": {
        "id": "3FX4_t__4wWK",
        "outputId": "74f74b74-2dc8-4106-e4a8-1e188c013a98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best sense for '(word)': emotion.n.01\n",
            "definition: any strong feeling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmed\"]\n",
        "\n",
        "print(\"Converting Word to its Stem\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "for w in words:\n",
        "    print(f\"{w} --> {ps.stem(w)}\")\n",
        "\n",
        "sentence = \"The programmers are programming a new program in Python\"\n",
        "\n",
        "tokenized_words = nltk.word_tokenize(sentence)\n",
        "\n",
        "print(\"\\nSentence Stemming:\")\n",
        "\n",
        "stemmed_sentence = [ps.stem(w) for w in tokenized_words]\n",
        "\n",
        "print(\" \".join(stemmed_sentence))\n"
      ],
      "metadata": {
        "id": "yvPQiNKH9HCQ",
        "outputId": "916896a3-4e04-45e7-ba6f-52b64a539d69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting Word to its Stem\n",
            "------------------------------\n",
            "program --> program\n",
            "programs --> program\n",
            "programmer --> programm\n",
            "programming --> program\n",
            "programmed --> program\n",
            "\n",
            "Sentence Stemming:\n",
            "the programm are program a new program in python\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_dict = {\n",
        "    \"cat\": \"NOUN\",\n",
        "    \"dog\": \"NOUN\",\n",
        "    \"child\": \"NOUN\",\n",
        "    \"run\": \"VERB\",\n",
        "    \"eat\": \"VERB\",\n",
        "    \"play\": \"VERB\",\n",
        "    \"happy\": \"ADJECTIVE\",\n",
        "    \"blue\": \"ADJECTIVE\",\n",
        "    \"i\": \"PRONOUN\",\n",
        "    \"you\": \"PRONOUN\"\n",
        "}\n",
        "\n",
        "word = input().lower()\n",
        "\n",
        "if word in pos_dict:\n",
        "    print(pos_dict[word])\n",
        "else:\n",
        "    print(\"UNKNOWN\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Pdm2dAsAEdsn",
        "outputId": "e2273e29-9e55-4c4e-8c20-b5e720692e2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "child\n",
            "NOUN\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}